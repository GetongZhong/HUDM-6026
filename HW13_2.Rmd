---
title: "HUDM 6026 HW 13"
author: "Frank Li & Getong Zhong"
date: "2023-04-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. 

```{r}
library(rpart)
library(rpart.plot)
set.seed(123)
n <- 100
x1 <- runif(n, 0, 10)
x2 <- runif(n, 0, 10)
assign_region <- function(x1, x2) {
  if (x1 < 3 & x2 < 5) return(0)
  if (x1 >= 3 & x1 < 6 & x2 < 5) return(1)
  if (x1 >= 6 & x2 < 5) return(2)
  if (x1 < 3 & x2 >= 5) return(3)
  if (x1 >= 3 & x1 < 6 & x2 >= 5) return(4)
  if (x1 >= 6 & x2 >= 5) return(5)
}
y <- mapply(assign_region, x1, x2)
df <- data.frame(x1, x2, y)
tree <- rpart(y ~ x1 + x2, data = df, method = "class")

rpart.plot(tree)
```

```{R}
grid_size <- 0.1
grid_x1 <- seq(min(df$x1), max(df$x1), by = grid_size)
grid_x2 <- seq(min(df$x2), max(df$x2), by = grid_size)
grid_df <- expand.grid(x1 = grid_x1, x2 = grid_x2)

# Predict the grid points using the decision tree
grid_df$y_pred <- predict(tree, grid_df, type = "class")
# Create a matrix of the predicted values for contour plot
pred_matrix <- matrix(grid_df$y_pred, nrow = length(grid_x2), ncol = length(grid_x1))

# Add the contour plot (decision boundary)
plot(df$x1, df$x2, col = y + 2, pch = 19)

# Add the contour plot (decision boundaries)
contour(grid_x1, grid_x2, t(pred_matrix), levels = c(0.5, 1.5, 2.5, 3.5, 4.5), drawlabels = FALSE, add = TRUE)
region_labels <- c("Region 1", "Region 2", "Region 3", "Region 4", "Region 5", "Region 6")
region_centers <- data.frame(x1 = c(1.5, 4.5, 8, 1.5, 4.5, 8), x2 = c(2.5, 2.5, 2.5, 7.5, 7.5, 7.5))
text(region_centers$x1, region_centers$x2, labels = region_labels, cex = 0.7, col = "black")
```


# 5. 
Majority vote approach:

There are 6 estimates of P > 0.5 so the final classification is red. 

Classify based on the average probability: 

```{r}
mean(c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75))
```

The average probability is 0.45 so the final classification is green. 

# 6. 
First the algorithm starts with the whole data set as the root node and repeatedly select the best split point to minimize the residuals. And then, the data partitioned based on the selected split points, and repeat this process for each child node until the end of the iteration, usually known as when the maximum depth are met. When we met the end of the iteration, terminal nodes are created, meanwhile, the mean of the target variable within the node is assigned to each child node.

# 8.
## (a)
```{r}
library (tree)
library (ISLR2)
attach (Carseats)
set.seed (123)
train <- sample (1: nrow (Carseats), nrow(Carseats)/2)
Carseats.train <- Carseats[train , ]
Carseats.test <- Carseats[-train , ]
```


## (b)
```{r}
tree.carseats <- tree (Sales ~., data = Carseats.train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

```{r}
tree.pred <- predict (tree.carseats , Carseats.test)
mean((tree.pred - Carseats.test$Sales)^2)
```

ShelveLoc and price are two variables that can best predict Sales since they 
appear on the top of the regression tree. The test MSE is 4.395357.

## (c)
```{r}
cv.carseats <- cv.tree(tree.carseats , FUN = prune.tree)
names (cv.carseats)
cv.carseats
```

```{r}
par (mfrow = c(1, 2))
plot (cv.carseats$size , cv.carseats$dev, type = "b")
plot (cv.carseats$k, cv.carseats$dev, type = "b")
prune.carseats <- prune.tree (tree.carseats , best = 5)
plot (prune.carseats)
text (prune.carseats , pretty = 0)
```


```{r}
tree.pred <- predict (prune.carseats , Carseats.test)
mean((Carseats.test$Sales - tree.pred)^2)
```

Pruning the tree DOES NOT improve the MSE, i.e. it goes from 4.395357 to 4.798268. 














