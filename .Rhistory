summary(sem, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
knitr::opts_chunk$set(echo = TRUE)
sem2 <- lavaan(model2, sample.cov = cor2, sample.nobs = 110, estimator = "ML")
cor2 <- matrix(c(
1.00, 0.37, 0.42, 0.53, 0.38, 0.81, 0.35, 0.42, 0.40, 0.24,
0.37, 1.00, 0.33, 0.14, 0.10, 0.34, 0.65, 0.32, 0.14, 0.15,
0.42, 0.33, 1.00, 0.38, 0.20, 0.49, 0.20, 0.75, 0.39, 0.17,
0.53, 0.14, 0.38, 1.00, 0.24, 0.58, -0.04, 0.46, 0.73, 0.15,
0.38, 0.10, 0.20, 0.24, 1.00, 0.32, 0.11, 0.26, 0.19, 0.43,
0.81, 0.34, 0.49, 0.58, 0.32, 1.00, 0.34, 0.46, 0.55, 0.24,
0.35, 0.65, 0.20, -0.04, 0.11, 0.34, 1.00, 0.18, 0.06, 0.15,
0.42, 0.32, 0.75, 0.46, 0.26, 0.46, 0.18, 1.00, 0.54, 0.20,
0.40, 0.14, 0.39, 0.73, 0.19, 0.55, 0.06, 0.54, 1.00, 0.16,
0.24, 0.15, 0.17, 0.15, 0.43, 0.24, 0.15, 0.20, 0.16, 1.00
), nrow = 10, ncol = 10, byrow = TRUE)
cor2
cor <- matrix(c(1, -0.04, 0.61, 0.45, 0.03, -0.29, -0.30, 0.45, 0.30,
-0.04, 1, -0.07, 0.59, 0.49, 0.43, 0.30, -0.31, -0.17,
0.61, -0.07, 1, -0.12, 0.03, -0.13, -0.24, 0.59, 0.32,
0.45, 0.59, -0.12, 1, 0.03, -0.13, -0.19, 0.63, 0.37,
0.03, 0.49, 0.03, 0.03, 1, 0.41, 0.41, -0.14, -0.24,
-0.29, 0.43, -0.13, -0.13, 0.41, 1, 0.63, -0.13, -0.29,
-0.30, 0.30, -0.24, -0.19, 0.41, 0.63, 1, -0.26, -0.29,
0.45, -0.31, 0.59, 0.63, -0.14, -0.13, -0.26, 1, 0.40,
0.30, -0.17, 0.32, 0.37, -0.24, -0.15, -0.29, 0.40, 1),
ncol = 9, byrow = TRUE)
cor
cor2
cor2 <- matrix(c(1, 0.37, 0.42, 0.53, 0.38, 0.81, 0.35, 0.42, 0.40, 0.24,
0.37, 1, 0.33, 0.14, 0.10, 0.34, 0.65, 0.32, 0.14, 0.15,
0.42, 0.33, 1, 0.38, 0.20, 0.49, 0.20, 0.75, 0.39, 0.17,
0.53, 0.14, 0.38, 1, 0.24, 0.58, -0.04, 0.46, 0.73, 0.15,
0.38, 0.10, 0.20, 0.24, 1, 0.32, 0.11, 0.26, 0.19, 0.43,
0.81, 0.34, 0.49, 0.58, 0.32, 1, 0.34, 0.46, 0.55, 0.24,
0.35, 0.65, 0.20, -0.04, 0.11, 0.34, 1, 0.18, 0.06, 0.15,
0.42, 0.32, 0.75, 0.46, 0.26, 0.46, 0.18, 1, 0.54, 0.20,
0.40, 0.14, 0.39, 0.73, 0.19, 0.55, 0.06, 0.54, 1, 0.16,
0.24, 0.15, 0.17, 0.15, 0.43, 0.24, 0.15, 0.20, 0.16, 1
), nrow = 10, ncol = 10, byrow = TRUE)
cor2
cor
cor2 <- matrix(c(1, 0.37, 0.42, 0.53, 0.38, 0.81, 0.35, 0.42, 0.40, 0.24,
0.37, 1, 0.33, 0.14, 0.10, 0.34, 0.65, 0.32, 0.14, 0.15,
0.42, 0.33, 1, 0.38, 0.20, 0.49, 0.20, 0.75, 0.39, 0.17,
0.53, 0.14, 0.38, 1, 0.24, 0.58, -0.04, 0.46, 0.73, 0.15,
0.38, 0.10, 0.20, 0.24, 1, 0.32, 0.11, 0.26, 0.19, 0.43,
0.81, 0.34, 0.49, 0.58, 0.32, 1, 0.34, 0.46, 0.55, 0.24,
0.35, 0.65, 0.20, -0.04, 0.11, 0.34, 1, 0.18, 0.06, 0.15,
0.42, 0.32, 0.75, 0.46, 0.26, 0.46, 0.18, 1, 0.54, 0.20,
0.40, 0.14, 0.39, 0.73, 0.19, 0.55, 0.06, 0.54, 1, 0.16,
0.24, 0.15, 0.17, 0.15, 0.43, 0.24, 0.15, 0.20, 0.16, 1
), ncol = 10, byrow = TRUE)
cor2
colnames(cor2) <- c("V1", "S1", "R1", "N1", "W1", "V2", "S2", "R2", "N2", "W2")
rownames(cor2) <- colnames(cor2)
cor_matrix_nearest_pd2 <- nearPD(cor2)$mat
cor_matrix_nearest_pd2
cor_matrix_nearest_pd2
model2 <- '
F1 =~ V1 + S1 + R1 + N1 + W1
F2 =~ V2 + S2 + R2 + N2 + W2
F1 ~~ F2'
sem2 <- lavaan(model2, sample.cov = cor2, sample.nobs = 110, estimator = "ML")
library(lavaan)
sem2 <- lavaan(model2, sample.cov = cor2, sample.nobs = 110, estimator = "ML")
sem2 <- lavaan(model2, sample.cov = as.matrix(cor_matrix_nearest_pd2), sample.nobs = 110, estimator = "ML")
cor <- matrix(c(1, -0.04, 0.61, 0.45, 0.03, -0.29, -0.30, 0.45, 0.30,
-0.04, 1, -0.07, 0.59, 0.49, 0.43, 0.30, -0.31, -0.17,
0.61, -0.07, 1, -0.12, 0.03, -0.13, -0.24, 0.59, 0.32,
0.45, 0.59, -0.12, 1, 0.03, -0.13, -0.19, 0.63, 0.37,
0.03, 0.49, 0.03, 0.03, 1, 0.41, 0.41, -0.14, -0.24,
-0.29, 0.43, -0.13, -0.13, 0.41, 1, 0.63, -0.13, -0.15,
-0.30, 0.30, -0.24, -0.19, 0.41, 0.63, 1, -0.26, -0.29,
0.45, -0.31, 0.59, 0.63, -0.14, -0.13, -0.26, 1, 0.40,
0.30, -0.17, 0.32, 0.37, -0.24, -0.15, -0.29, 0.40, 1),
ncol = 9, byrow = TRUE)
colnames(cor) <- c("Q1", "Q2", "Q3", "Q4", "Q5", "Q6", "Q7", "Q8", "Q9")
rownames(cor) <- colnames(cor)
cor_matrix_nearest_pd <- nearPD(cor)$mat
cor_matrix_nearest_pd
model <- '
DoctorResponsibility =~ Q1 + Q3 + Q4 + Q8
PatientResponsibility =~ Q2 + Q5 + Q6 + Q7
DoctorResponsibility ~~ PatientResponsibility'
sem <- sem(model, sample.cov = as.matrix(cor_matrix_nearest_pd) ,sample.nobs = 123, estimator = "ML")
summary(sem, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
sem2 <- sem(model2, sample.cov = as.matrix(cor_matrix_nearest_pd2), sample.nobs = 110, estimator = "ML")
summary(sem2, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
colnames(cor) <- c("anomia_1967", "powerlessness_1967", "anomia_1971", "powerlessness_1971")
cor <- matrix(c(
1.0, 0.6, 0.5, 0.4,
0.6, 1.0, 0.7, 0.5,
0.5, 0.7, 1.0, 0.8,
0.4, 0.5, 0.8, 1.0
), nrow = 4, byrow = TRUE)
colnames(cor) <- c("anomia_1967", "powerlessness_1967", "anomia_1971", "powerlessness_1971")
rownames(cor) <- colnames(cor)
model3 <- '
model3 <- '
# Latent variables
Alienation_1967 =~ anomia_1967 + powerlessness_1967
Alienation_1971 =~ anomia_1971 + powerlessness_1971
# Correlated errors
anomia_1967 ~~ anomia_1971
sem3 <- sem(model, sample.cov = cor, sample.nobs = n, estimator = "ML")
sem3 <- sem(model3, sample.cov = cor, sample.nobs = n, estimator = "ML")
sem3 <- sem(model3, sample.cov = cor, sample.nobs = 110, estimator = "ML")
sem3 <- sem(model3, sample.cov = cor, sample.nobs = 100, estimator = "ML")
model3 <- '
# Latent variables
Alienation_1967 =~ anomia_1967 + powerlessness_1967
Alienation_1971 =~ anomia_1971 + powerlessness_1971
# Correlated errors
anomia_1967 ~~ anomia_1971
'
sem3 <- sem(model3, sample.cov = cor, sample.nobs = 100, estimator = "ML")
summary(fit, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
summary(sem3, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
knitr::opts_chunk$set(echo = TRUE)
set.seed(6026)
x1 <- runif(100)
x2 <- runif(100)
t1 <- 0.3
t2 <- 0.6
# Partition the feature space using recursive binary splitting
R1 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 < t2)
R2 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 >= t2)
R3 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 < t2)
R4 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 >= t2)
# Plot the partitioned feature space
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue", "yellow")))))
abline(v=t1, lty=2)
# Plot the partitioned feature space
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue", "yellow")))))
# Partition the feature space using recursive binary splitting
R1 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 < t2)
R2 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 >= t2)
R3 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 < t2)
R4 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 >= t2)
X1 <- runif(100)
X2 <- runif(100)
t1 <- 0.3
t2 <- 0.6
# Partition the feature space using recursive binary splitting
R1 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 < t2)
R2 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 >= t2)
R3 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 < t2)
R4 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 >= t2)
# Plot the partitioned feature space
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue", "yellow")))))
ifelse(X1 >= t1 & X2 < t2, "blue", "yellow")
abline(h=t2, lty=2)
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue", "yellow")
abline(v=t1, lty=2)
# Plot the partitioned feature space
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue", "yellow")))
abline(v=t1, lty=2)
install.packages("rpart.plot")
library(rpart.plot)
install.packages("rpart")
install.packages("rpart")
library(rpart)
df <- data.frame(X1=X1, X2=X2)
tree_fit <- tree(X1 + X2 ~ 1, data=df)
library(tree)
install.packages("tree")
library(tree)
set.seed(6026)
X1 <- runif(100)
X2 <- runif(100)
df <- data.frame(X1=X1, X2=X2)
tree_fit <- tree(X1 + X2 ~ 1, data=df)
tree_fit <- tree(X1 + X2 ~ ., data=df, subset=c(-R1, -R2, -R3, -R4))
# Plot the decision tree using the plot() function
plot(tree_fit)
tree_fit <- tree(X1 + X2 ~ ., data=df, subset=c(-R1, -R2, -R3, -R4))
df <- data.frame(X1=X1, X2=X2, Y=ifelse(X1 < t1 & X2 < t2, "R1",
ifelse(X1 < t1 & X2 >= t2, "R2",
ifelse(X1 >= t1 & X2 < t2, "R3", "R4"))))
# Fit a decision tree using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data=df)
# Plot the decision tree using the plot() function
plot(tree_fit)
# Fit a decision tree using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data = df, splitrule="deviance")
set.seed(123)
X1 <- runif(100)
X2 <- runif(100)
# Define the partition boundaries
t1 <- 0.3
t2 <- 0.6
# Partition the feature space using recursive binary splitting
R1 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 < t2)
R2 <- subset(data.frame(X1=X1, X2=X2), X1 < t1 & X2 >= t2)
R3 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 < t2)
R4 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 >= t2)
# Plot the partitioned feature space
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue",
"orange")))))
plot(X1, X2, pch=16, col=ifelse(X1 < t1 & X2 < t2, "red",
ifelse(X1 < t1 & X2 >= t2, "green",
ifelse(X1 >= t1 & X2 < t2, "blue",
"orange"))))
abline(v=t1, lty=2)
abline(h=t2, lty=2)
df <- data.frame(X1=X1, X2=X2, Y=ifelse(X1 < t1 & X2 < t2, "R1",
ifelse(X1 < t1 & X2 >= t2, "R2",
ifelse(X1 >= t1 & X2 < t2, "R3",
"R4")))))
df <- data.frame(X1=X1, X2=X2, Y=ifelse(X1 < t1 & X2 < t2, "R1",
ifelse(X1 < t1 & X2 >= t2, "R2",
ifelse(X1 >= t1 & X2 < t2, "R3",
"R4"))))
tree_fit <- tree(Y ~ X1 + X2, data=df)
plot(tree_fit)
# Partition the feature space using recursive binary splitting
R1 <- subset(data.frame(X1=X1, X2=X2), X1 < t1)
R2 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 < t2)
R3 <- subset(data.frame(X1=X1, X2=X2), X1 >= t1 & X2 >= t2)
# Plot the partitioned feature space
plot(X1, X2, pch=16, col=ifelse(X1 < t1, "red",
ifelse(X1 >= t1 & X2 < t2, "green",
"blue"))))
plot(X1, X2, pch=16, col=ifelse(X1 < t1, "red",
ifelse(X1 >= t1 & X2 < t2, "green",
"blue")))
abline(v=t1, lty=2)
abline(h=t2, lty=2)
df <- data.frame(X1=X1, X2=X2, Y=ifelse(X1 < t1, "R1",
ifelse(X1 >= t1 & X2 < t2, "R2",
"R3")))
tree_fit <- tree(Y ~ X1 + X2, data=df)
# Plot the decision tree using the plot() function
plot(tree_fit)
df <- data.frame(X1=0.5, X2=0.5, Y="R1")
# Fit a decision tree to the data using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data=df)
# Plot the decision tree using the plot() function
plot(tree_fit)
X1 <- runif(100)
X2 <- runif(100)
Y <- ifelse(X1 < 0.3, "R1", ifelse(X1 < 0.6 & X2 < 0.5, "R2", "R3"))
df <- data.frame(X1=X1, X2=X2, Y=Y)
# Fit a decision tree to the data using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data=df)
# Plot the decision tree using the plot() function
plot(tree_fit)
# Fit a decision tree to the data using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data=df)
df <- data.frame(X1=X1, X2=X2, Y=Y)
# Fit a decision tree to the data using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data=df)
# Plot the decision tree using the plot() function
plot(tree_fit)
set.seed(123)
X1 <- runif(100)
X2 <- runif(100)
Y <- ifelse(X1 < 0.3, "R1", ifelse(X1 < 0.6 & X2 < 0.5, "R2", "R3"))
df <- data.frame(X1=X1, X2=X2, Y=Y)
# Fit a decision tree to the data using the tree() function
tree_fit <- tree(Y ~ X1 + X2, data=df)
# Plot the decision tree using the plot() function
plot(tree_fit)
library(rpart)
# Create a sample dataset
data(iris)
# Build a decision tree model using the tree function
model <- tree(Species ~ ., data = iris)
# Plot the decision tree
plot(model)
# Generate a random dataset with three predictors and one target variable
n <- 1000
x1 <- rnorm(n)
x2 <- rbinom(n, 1, 0.5)
x3 <- rpois(n, 2)
y <- as.factor(rbinom(n, 1, 0.3))
# Combine the predictors and target variable into a data frame
data <- data.frame(x1, x2, x3, y)
# Build a decision tree model using the rpart function
library(rpart)
model <- rpart(y ~ ., data = data)
# Plot the decision tree
plot(model)
# Plot the decision tree
plot(model)
model <- tree(y ~ ., data = data)
# Plot the decision tree
plot(model)
x1 <- runif(100, 0, 10)
x2 <- runif(100, 0, 10)
y <- ifelse(x1 > 5 & x2 > 5, "A", "B")
data <- data.frame(x1, x2, y)
# Build decision tree
library(tree)
tree <- tree(y ~ x1 + x2, data = data)
# Plot decision tree
plot(tree)
text(tree)
# Create partition
t1 <- 5
t2 <- 5
t3 <- 3
t4 <- 7
R1 <- subset(data, x1 <= t1 & x2 <= t2)
R2 <- subset(data, x1 <= t1 & x2 > t2)
R3 <- subset(data, x1 > t1 & x2 <= t2)
R4 <- subset(data, x1 > t1 & x2 > t2)
R5 <- subset(data, x1 <= t3 & x2 > t1 & x2 <= t4)
R6 <- subset(data, x1 > t3 & x2 > t1 & x2 <= t4)
tree$frame
knitr::opts_chunk$set(echo = TRUE)
library(tree)
set.seed(123)
x1 <- runif(100, 0, 10)
x2 <- runif(100, 0, 10)
y <- ifelse(x1 > 5 & x2 > 5, "A", "B")
data <- data.frame(x1, x2, y)
tree <- tree(y ~ x1 + x2, data = data)
# Plot decision tree
plot(tree)
text(tree)
tree$frame
data
n <- 100
x1 <- runif(n, 0, 10)
x2 <- runif(n, 0, 10)
y <- ifelse(x1 + x2 > 10, 1, 0)
df <- data.frame(x1, x2, y)
# Fit a decision tree with a maximum depth of 3
tree <- tree(y ~ x1 + x2, data = df, maxdepth = 3)
# Fit a decision tree with a maximum depth of 3
tree <- tree(y ~ x1 + x2, data = df)
# Plot the resulting tree
plot(tree)
text(tree, pretty = 0)
plot(tree)
text(tree, pretty = 0)
plot(tree)
text(tree)
library(mvtnorm)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
### Generate checkerboard data
set.seed(156709)
X1 <- runif(200, -1, 1); X2 <- runif(200, -1, 1)
plot(X1, X2)
Y <- ifelse(X1 < -.5 & X2 > 0 | X1 > -.5 & X1 < 0 & X2 < 0 |
X1 > 0 & X1 < .5 & X2 > 0 | X1 > .5 & X2 < 0, 0, 1)
df <- data.frame(cbind(X1, X2, Y))
head(df)
plot(X1, X2, col = Y + 2, pch = 19)
plot(X1, X2, col = Y , pch = 19)
plot(X1, X2, col = Y + 2, pch = 19)
### Estimate separation boundary with logistic regression
summary(glm1 <- glm(Y ~ X1 + X2, family = "binomial"))
newdat <- data.frame(expand.grid(seq(-1,1,.05), seq(-1,1,.05)))
names(newdat) <- c("X1", "X2")
prob <- matrix(predict(glm1, newdata = newdat), 41, 41)
contour(seq(-1, 1, .05), seq(-1, 1, .05), z = prob, levels = 0.5, labels = "",
xlab = "", ylab = "", axes = FALSE, add = TRUE, lwd = 3,
col = "cornflowerblue")
### With 2nd order logistic regression
plot(X1, X2, col = Y + 2, pch = 19)
summary(glm2 <- glm(Y ~ (X1 + I(X1^2))*(X2 + I(X2^2)), family = "binomial"))
prob2 <- matrix(predict(glm2, newdata = newdat), 41, 41)
contour(seq(-1, 1, .05), seq(-1, 1, .05), z = prob2, levels = 0.5, labels = "",
xlab = "", ylab = "", axes = FALSE, add = TRUE, lwd = 3,
col = "cornflowerblue")
### With 3rd order logistic regression
plot(X1, X2, col = Y + 2, pch = 19)
summary(glm3 <- glm(Y ~ (X1 + I(X1^2) + I(X1^3))*(X2 + I(X2^2) + I(X2^3)),
family = "binomial", control = glm.control(maxit = 500)))
prob3 <- matrix(predict(glm3, newdata = newdat), 41, 41)
contour(seq(-1, 1, .05), seq(-1, 1, .05), z = prob3, levels = 0.5, labels = "",
xlab = "", ylab = "", axes = FALSE, add = TRUE, lwd = 3,
col = "cornflowerblue")
cart1 <- rpart(Y ~ X1 + X2, data = df, method = "class")
rpart.plot(cart1)
col0 <- adjustcolor("red", alpha = .2)
cart1 <- rpart(Y ~ X1 + X2, data = df, method = "class")
rpart.plot(cart1)
set.seed(123)
n <- 100
x1 <- runif(n, 0, 10)
x2 <- runif(n, 0, 10)
y <- ifelse(x1 + x2 > 10, 1, 0)
df <- data.frame(x1, x2, y)
tree <- rpart(y ~ x1 + x2, data = df, method = "class")
rpart.plot(tree)
col0 <- adjustcolor("red", alpha = .2)
col1 <- adjustcolor("green", alpha = .2)
plot(X1, X2, col = Y + 2, pch = 19) # cart1
polygon(c(-1.1, 1.1, 1.1, -1.1), y = c(-1.1, -1.1, 1.1, 1.1), col = col1) # cart2
plot(X1, X2, col = Y + 2, pch = 19) # cart3
polygon(x = c(-1.1, 1.1, 1.1, -1.1), y = c(.028, .028, 1.1, 1.1), border = "black",
col = col0)
polygon(x = c(-1.1, 1.1, 1.1, -1.1), y = c(.028, .028, -1.1, -1.1), border = "black",
col = col1)
plot(X1, X2, col = Y + 2, pch = 19) # cart4
polygon(x = c(-1.1, .54, .54, -1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(-1.1, 1.1, 1.1, -1.1), y = c(.028, .028, -1.1, -1.1), border = "black",
col = col1)
polygon(x = c(1.1, .54, .54, 1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col1)
plot(X1, X2, col = Y + 2, pch = 19) # cart5
polygon(x = c(.012, .54, .54, .012), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(.012, -1.1, -1.1, .012), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(-1.1, 1.1, 1.1, -1.1), y = c(.028, .028, -1.1, -1.1), border = "black",
col = col1)
polygon(x = c(1.1, .54, .54, 1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col1)
plot(X1, X2, col = Y + 2, pch = 19) # cart6
polygon(x = c(.012, .54, .54, .012), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(.012, -.475, -.475, .012), y = c(1.1, 1.1, .028, .028), border = "black",
col = col1)
polygon(x = c(-1.1, -.475, -.475, -1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(-1.1, 1.1, 1.1, -1.1), y = c(.028, .028, -1.1, -1.1), border = "black",
col = col1)
polygon(x = c(1.1, .54, .54, 1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col1)
plot(X1, X2, col = Y + 2, pch = 19) # cart7
polygon(x = c(.012, .54, .54, .012), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(.012, -.475, -.475, .012), y = c(1.1, 1.1, .028, .028), border = "black",
col = col1)
polygon(x = c(-1.1, -.475, -.475, -1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(1.1, .54, .54, 1.1), y = c(1.1, 1.1, .028, .028), border = "black",
col = col1)
polygon(x = c(1.1, -.505, -.505, 1.1), y = c(-1.1, -1.1, .028, .028), border = "black",
col = col0)
polygon(x = c(-1.1, -.505, -.505, -1.1), y = c(-1.1, -1.1, .028, .028), border = "black",
col = col1)
### 3d plot classification example
x1 <- x2 <- seq(-1, 1, .05)
dom <- expand.grid(x1, x2)
fout <- function(x, y) {
if (y >= 0 & x <= -.5) {out <- 0}
if (y >= 0 & x >= -.5 & x <= 0) {out <- 1}
if (y >= 0 & x >= 0 & x <= .5) {out <- 0}
if (y >= 0 & x >= .5) {out <- 1}
if (y < 0 & x <= -.5) {out <- 1}
if (y < 0 & x >= -.5 & x <= 0) {out <- 0}
if (y < 0 & x >= 0 & x <= .5) {out <- 1}
if (y < 0 & x >= .5) {out <- 0}
out
}
output <- numeric(dim(dom)[1])
for (i in 1:dim(dom)[1]) {output[i] <- fout(dom[i,1], dom[i,2])}
library(rgl)
plot3d(x = dom[,1], y = dom[,2], z = output, type = "p")
cbind(dom[,1], dom[,2], output)
cart3 <- rpart(output ~ dom[,1] + dom[,2])
rpart.plot(cart3)
rpart.plot(cart3)
cart3
### Sine curve example for regression trees
set.seed(202393)
x <- runif(100, -pi, pi)
y <- sin(x) + rnorm(100, sd = .3)
plot(x, y)
# Consider splitting at x = -1.5.
(c1 <- mean(y[x < -1.5]))
(c2 <- mean(y[x >= -1.5]))
(loss <- sum((y[x < -1.5] - c1)^2) + sum((y[x >= -1.5] - c2)^2))
# Consider splitting at x = 0.0.
(c1 <- mean(y[x < 0]))
(c2 <- mean(y[x >= 0]))
(loss <- sum((y[x < 0] - c1)^2) + sum((y[x >= 0] - c2)^2))
cart4 <- rpart(y ~ x)
cart4
plot(x, y)
predvals <- predict(cart4, newdata = data.frame(x = seq(-3, 3, .01)))
points(seq(-3, 3, .01), predvals, type = "l", lwd = 2)
rf1 <- randomForest(y ~ x)
predvalsrf1 <- predict(rf1, newdata = data.frame(x = seq(-3, 3, .01)))
plot(x, y)
points(seq(-3, 3, .01), predvalsrf1, type = "l", lwd = 3)
### Cost-complexity tuning
# Re-do cart4 but with no complexity pruning.
cart5 <- rpart(y ~ x, cp = .001, minbucket = 2)
rpart.plot(cart4)
rpart.plot(cart5)
plotcp(cart5)
printcp(cart5)
# Refit with optimal cp value selected.
cart6 <- rpart(y ~ x, cp = .01, minbucket = 2)
rpart.plot(cart6)
plot(x, y)
predvals2 <- predict(cart5, newdata = data.frame(x = seq(-3, 3, .01)))
points(seq(-3, 3, .01), predvals2, type = "l", lwd = 2)
### Random Forests with ECLSK data.
load(file = file.choose()) # load the eclsk_sim data set
